---
title: "DoD Contract Overages"
authors: "Bryan Baird, Loren Lipsey, Tommie Thompson"
date: "May 8, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Predicting Contract Terminations at the U.S. Department of Defense

## Introduction

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

## Outside Packages
```{r, message=FALSE}
library(randomForest)
library(dplyr)
library(caret)
```

## Loading and Cleaning Data

The data used in this analysis have already been aggregated and somewhat cleaned; they are loaded from our online repository, which also includes the scripts used to access and aggregate the raw data. For more information on the data source, see the README file.

```{r}
#load data
rm(list=ls(all=TRUE))
githubURL <- "https://github.com/GeorgetownMcCourt/DoD-over-budget-prediction/blob/master/clean_dataset.Rda"
download.file(githubURL,"myfile.Rda")
source("myfile.Rda")
df <- dataset
```


## Functions Used

In addition to the functions included in the packages already loaded in, our analysis also makes use of two funtions to check the accuracy of predictions. The first calculates the Mean F1 score, which is a measure of predictive accuracy that ranges from 0 to 1. The target for reliable predictions in this case is somewhere in the area of 0.8 and above, but values very close to 1 could be a sign of overfitting in the test process.

```{r}
#Mean
meanf1 <- function(actual, predicted){
  #Mean F1 score function
  #actual = a vector of actual labels
  #predicted = predicted labels
  
  classes <- unique(actual)
  results <- data.frame()
  for(k in classes){
    results <- rbind(results, 
                     data.frame(class.name = k,
                                weight = sum(actual == k)/length(actual),
                                precision = sum(predicted == k & actual == k)/sum(predicted == k), 
                                recall = sum(predicted == k & actual == k)/sum(actual == k)))
  }
  results$score <- results$weight * 2 * (results$precision * results$recall) / (results$precision + results$recall) 
  return(sum(results$score))
}
```

The second custom function is simply a shortcut for applying the *meanf1* function to different subpopulations, simplifying the Train-Validate-Test process.

```{r}
#Simple predict function 
pred.check <- function(x,y) { 
  #"x" is the partition for validation and "y" is the model 
  #so I only need to modify one model when checking 
  pred <- predict(y, x, type="class")
  z <- meanf1(x$IsTerminated, pred) 
  print(z)
}
```

The third function is to simply calculate the "Area Under the Curve", or *AUC*, score. AUC represents a slightly different way of measuring accuracy than Mean F1, but both measure on the same scale of 0--1. For a binary outcome like *IsTerminated*, the difference between the AUC and 0.5 is the extent to which the model predicts outcomes more accurately than a random coin flip.

```{r}
#simple AUC function
auc.check <- function(x,y) { 
  #"x" is the partition for validation and "y" is the model 
  pred <- predict(y, x, type="class")
  #convert to numeric binary so auc can be calculated
  pred <- ifelse(pred == "Terminated", 1, 0)
  real <- ifelse(x$IsTerminated == "Terminated", 1, 0)
  #calculate auc
  z <- auc(real, pred) 
  print(z)
}
```

## Data Cleaning

As this is a case of supervised learning, our analysis cannot make use of data points that do not have a known outcome, the variable *IsTerminated*. (They can be predicted, but not verified, making them functionally equivalent to observations outside of the data set.) These observations are dropped.

```{r}
#remove missing from outcome variables
df <- df[!is.na(df$IsTerminated),]
```

For the sake of parsimony, we also drop variables that are not included in out analysis, either because they are not predictive (e.g. the contract ID) or missing so many observations that their inclusion would severely dimish sample size without imputation.

```{r}
#subset relevant variables (drop irrelevant, date-based, and more than 1mm missing)
drop <- names(df) %in% c("CSIScontractID", "StartFiscal_Year", "MinOfSignedDate", 
                         "MinOfEffectiveDate", "unmodifiedSystemequipmentcode",
                         "UnmodifiedUltimateCompletionDate", "UnmodifiedLastDateToOrder",
                         "Unmodifiedmultipleorsingleawardidc", "UnmodifiedLastDateToOrder",
                         "UnmodifiedIsOnlyOneSource", "UnmodifiedIsFollowonToCompetedAction",
                        "UnmodifiedIsCombination", "Unmodifiedaddmultipleorsingawardidc",
                        "UnmodifiedCustomer","UnmodifiedIsFullAndOpen", "UnmodifiedIsSomeCompetition",
                        "UnmodifiedNumberOfOffersReceived",
      #UnmodifiedPlaceCountryISO3 has too many levels. How do fix??
                        "UnmodifiedPlaceCountryISO3", "pChangeOrderUnmodifiedBaseAndAll")
df <- df[!drop]
```

Some variables are recoded to be a more amenenable type in R, especially to de-factorize variables that R reads as factors by default. For some variables with a significant number of *NA* values, *NA* is recoded as a factor level to accomodate any predictive power that may be associated with failing to have that particular value. For example, it is possible that a factor being improperly or insufficiently documented in some cases might be associated with higher risk of other organizational problems that could lead to contract termination.

```{r}
#change class
df$UnmodifiedDays <- as.numeric(df$UnmodifiedDays)
df$UnmodifiedCurrentCompletionDate <- months(df$UnmodifiedCurrentCompletionDate)
df$UnmodifiedAwardOrIDVcontractactiontype <- as.character(df$UnmodifiedAwardOrIDVcontractactiontype)
df$UnmodifiedSubCustomer <- as.character(df$UnmodifiedSubCustomer)

#recode NA to dummy for those with >100k NA
#Removed vars: #df$UnmodifiedIsFullAndOpen[is.na(df$UnmodifiedIsFullAndOpen)] <- 3
#df$UnmodifiedIsSomeCompetition[is.na(df$UnmodifiedIsSomeCompetition)] <- 2
#df$UnmodifiedNumberOfOffersReceived[is.na(df$UnmodifiedNumberOfOffersReceived)] <- 1000
df$UnmodifiedAwardOrIDVcontractactiontype[is.na(df$UnmodifiedAwardOrIDVcontractactiontype)] <- "Unknown"
df$UnmodifiedSubCustomer[is.na(df$UnmodifiedSubCustomer)] <- "Unknown"

#recode outcome IsTerminated with characters
df$IsTerminated <- ifelse(df$IsTerminated == 0, "Terminated", "Not Terminated")

#turn all char into factor
for(i in colnames(df)) {
  x <- class(df[,i])
  if(x == 'character') {
    df[,i] <- as.factor(df[,i])
  }  
}

#turn all int into num
for(i in colnames(df)) {
  x <- class(df[,i])
  if(x != 'factor') {
    df[,i] <- as.numeric(df[,i])
  }  
}

```


```{r}
#too lazy to type out all variables in quotations. Doing it quickly here
cat(colnames(df))#copy and paste this into text line

var <- "LabeledMDAP UnmodifiedIsInternational UnmodifiedContractObligatedAmount UnmodifiedContractBaseAndExercisedOptionsValue UnmodifiedContractBaseAndAllOptionsValue UnmodifiedCurrentCompletionDate IsTerminated UnmodifiedDays UnmodifiedPlatformPortfolio UnmodifiedProductOrServiceArea UnmodifiedSimpleArea UnmodifiedAwardOrIDVcontractactiontype UnmodifiedSubCustomer UnmodifiedTypeOfContractPricing UnmodifiedIsFixedPrice UnmodifiedIsCostBased UnmodifiedIsIncentive UnmodifiedIsAwardFee UnmodifiedIsFFPorNoFee UnmodifiedIsFixedFee UnmodifiedIsOtherFee"
gsub(" ", " + ", var)#copy and paste this into formula
cat(gsub("(\\w+)", '"\\1"', var))
```


Finally, we remove missing cases since our analytical methods cannot handle them. We also balance the dataset on the outcome variable. Before balancing, the size of "Not Terminated" observations was only 1% that of "Terminated" cases. We balanced the ratio to 50%. The final dataset, after balancing the outcome and remove irrelevant data is 86,586. This is a big drop from the original size of over two million, but without balancing we would have had an overconfident model. 

```{r}
#can only use complete cases
df_comp <- df[complete.cases(df),]

#fix balance
table(df_comp$IsTerminated)

df_term <- df_comp[df_comp$IsTerminated == "Terminated",]
df_non <- df_comp[df_comp$IsTerminated == "Not Terminated",]

set.seed(3)
downsample <- df_term[sample(nrow(df_term),57724),]

df_balanced <- rbind(df_non,downsample)
```


## Partitioning: Train-Validate-Test

The available data are randomly split into three groups: 70% of observations go into a training set, 15% go to a validate set to check the predictive accuracy of the trained model, and a final 15% for a final test of the model settled on after experimenting with the train-validate combination.

Note that while the bootstrapped nature of the Random Forest approach does provide a certain amount of "Out of Bag" cases for free testing, but with the large number of observations we have available, we safely proceed with the more rigourous formal partitioniong.

```{r}

{
  set.seed(100)
  sample_train <- floor(.7*nrow(df_balanced))
  sample_valid <- floor(.15*nrow(df_balanced))
  
  index_train <- sort(sample(seq_len(nrow(df_balanced)), size=sample_train))
  index_not.train <- setdiff(seq_len(nrow(df_balanced)), index_train)
  index_valid  <- sort(sample(index_not.train, size=sample_valid))
  index_test  <- setdiff(index_not.train, index_valid)
  
  df_train  <- df_balanced[index_train, ]
  df_valid <- df_balanced[index_valid, ]
  df_test <- df_balanced[index_test, ]
}
```

## Model: Random Forest

Our model of choice is a Random Forest using the 9 variables listed to make a classification prediction of yes or no for the outcome variable *IsTerminated*. The model is run on the training set *df_train*.

The configuration shown here, consisting of 9 input variables, and 100 trees, is the final result of the iteration of trying different configurations of the Train-Validate loop. The number of decision trees is set to only 100 instead of the default 500 because the *tuneRF* function below shows that there is little precision benefit after the first 100 trees, despite increasing computational costs.

```{r}
###random forest classification
set.seed(123)
rf <- randomForest(IsTerminated ~ 
                     UnmodifiedIsInternational + 
                     UnmodifiedCurrentCompletionDate + 
                     UnmodifiedDays + 
                     UnmodifiedPlatformPortfolio + 
                     UnmodifiedProductOrServiceArea + 
                     UnmodifiedSimpleArea + 
                     UnmodifiedAwardOrIDVcontractactiontype + 
                     UnmodifiedSubCustomer + 
                     UnmodifiedTypeOfContractPricing, 
                   data=df_train, 
                   mtry = 2, ntree = 100, 
                   #omit missing values
                   na.action = na.omit, importance = T, 
                   type = "classification")
```

Because the *importance* paramater above is set to TRUE, we can display the relative importance of different input variables in the predicted outcomes of the model. The values shown have little directly intepretable value, but shows that *UnmodifiedDays*, *UnmodifiedPlatformPortfolio*, and *UnmodifiedProductOrServiceArea* show up as crticial variables more often than other input factors. This means that the length of the contract and the type of work undertaken are more likely to predict a contract's termination than the included details of the contractor or whether international work is involved.

```{r}
#check importance of each variable
imp <- as.data.frame(sort(importance(rf)[,1],decreasing = TRUE),optional = T)
names(imp) <- "% Inc MSE"
imp

varImpPlot(rf)
```

As discussed above, *tuneRF* here produces a chart showing that there is relatively little benefit to additional computationally-intensive trees after the first 100 or so.

```{r}
#find optimal mtry 
tuneRF(df_train[,
                #put in all predictors in final RF model
                c("IsTerminated","UnmodifiedIsInternational","UnmodifiedCurrentCompletionDate",
                  "UnmodifiedDays","UnmodifiedPlatformPortfolio",
                  "UnmodifiedProductOrServiceArea","UnmodifiedSimpleArea",
                  "UnmodifiedAwardOrIDVcontractactiontype","UnmodifiedSubCustomer",
                  "UnmodifiedTypeOfContractPricing")], 
       #put in outcome
       df_train$IsTerminated, 
       ntreeTry = 400, na.action = na.omit, 
       mtryStart = 1, stepFactor = 2, 
       improve = 0.001, trace = TRUE, plot = F)

```

## Measuring Model Output

With this trained Random Forest model in hand, we can now evaluate its accuracy in predicting outcome values against not only itself (as an internal consistency check) and the validate set, but also the holdout test set.

```{r}
pred.check(df_train, rf)
pred.check(df_valid, rf)
pred.check(df_test, rf)
```

From this, we see a stable Mean F1 of around 0.85 for predictions in all three sets, a decently high score that 
roughly means that the model can predict contract terminations with 85% accuracy.

Similarly, the AUC scores are favorably high and stable between train, validate, and test, suggesting a relatively robust predictive capability without any sign of overfitting.

```{r}
auc.check(df_train, rf)
auc.check(df_valid, rf)
auc.check(df_test, rf)
```

## Conclusion and Next Steps

In conclusion, using only 9 input factors out of dozens of possible factors, a Random Forest model is able to predict whether not a Defense contract will be terminated with 85% accuracy. 

### Extending analysis to other populations

The set of observations used for this proof of concept is very restricted to start. Of millions of total contracts, only 82,000 meet all of the criteria for inclusion, raising the risk of weaker than expected predictive power when working with cases that are even just slightly different from the defined set.

### Further cleaning to maximize both variables and observations

More fine-grain tuning, especially around the many missing values in the aggregated data, would increase the predictive power and reliability of the model in two key ways: First, the integrity of more input variables could be more throughly established, allowing them to be easily added to the Random Forest without fear of endogeniety. Second, fewer observations would need to be dropped to run the model, increasing presicion and generalizability by taking full advantage of as many example contracts as possible.

### Predicting other risk categories

With some additional data cleaning and investigation, it should be possible to reliably apply a categorical variable to all contracts indicating whether or not the contract went over budget, and if so, a numerical variable showing by how much the budget was exceeded. If these values can be shown to be reliable at the contract level, it would be a straightforward endeavour to repurporse the Random Forest model used here to predict overages instead of terminations.